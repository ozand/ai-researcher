"Level 1","Level 2","Level 3","Level 4","Level 5"
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Core Steps (The Pipeline)","1. Generate Baseline Response (Initial Draft)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Core Steps (The Pipeline)","2. Plan Verifications (Generate Fact-Checking Questions)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Core Steps (The Pipeline)","3. Execute Verifications (Answer Questions Independently)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Core Steps (The Pipeline)","4. Generate Final Verified Response (Incorporate Corrections)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Verification Execution Variants","Joint (Plan + Execute in one prompt, risks repetition)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Verification Execution Variants","2-Step (Separate prompts, execution context excludes baseline)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Verification Execution Variants","Factored (Independent prompts per question, high performance)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Verification Execution Variants","Factor+Revise (Factored + Explicit cross-check for inconsistency)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Key Findings & Benefits","Decreases Hallucinations (Reduces negative entities)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Key Findings & Benefits","Improves Precision (across various tasks)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Key Findings & Benefits","Factored/2-Step Variants improve performance (Avoids repetition bias)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Key Findings & Benefits","Shortform Verification Questions are more Accurate",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Key Findings & Benefits","Outperforms LLama 65B Baseline, ChatGPT, and InstructGPT (Longform)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Limitations","Does not remove hallucinations completely",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Limitations","Focuses only on factual inaccuracies",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Limitations","Increased computational expense (more tokens/prompts)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Limitations","Upper bound limited by base LLM capability",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Evaluation Tasks (Benchmarks)","Wikidata (List-based questions)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Evaluation Tasks (Benchmarks)","Wiki-Category List (Harder set generation)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Evaluation Tasks (Benchmarks)","MultiSpanQA (Closed-book factoid QA)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Chain-of-Verification (CoVe) Method","Evaluation Tasks (Benchmarks)","Longform Generation (Biographies, FACTSCORE metric)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Mechanism and Purpose","Enhances LLMs with external knowledge sources",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Mechanism and Purpose","Retrieves relevant info in real-time",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Mechanism and Purpose","Augments model response with up-to-date/domain-specific facts",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Mechanism and Purpose","Mitigates hallucinations caused by knowledge gaps",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Technical Components","Information Retrieval (IR)","Source Documents (PDF, Word, Excel)"
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Technical Components","Information Retrieval (IR)","LLM converts query to embeddings"
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Technical Components","Information Retrieval (IR)","Vector DB Similarity Search"
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Technical Components","Information Retrieval (IR)","Related Chunks Passed to Prompt"
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Technical Components","Natural Language Generation (NLG)","LLM generates text from augmented prompt"
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Technical Components","Natural Language Generation (NLG)","Includes Guardrail and Post-process steps"
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","RAG Implementation Types (Mitigation Section)","One-time Retrieval (Prepend knowledge to prompt)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","RAG Implementation Types (Mitigation Section)","Iterative Retrieval (Knowledge gathering throughout generation, e.g., ReAct)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","RAG Implementation Types (Mitigation Section)","Post-hoc Retrieval (Refine output via subsequent retrieval-based revisions, e.g., RARR)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Challenges in RAG (Generation Bottleneck)","Retrieval Failure (Query ambiguity, unreliable sources, chunking/embedding issues)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Challenges in RAG (Generation Bottleneck)","Noisy Context (Irrelevant info misleads generator)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Challenges in RAG (Generation Bottleneck)","Context Conflict (Parametric knowledge vs. contextual knowledge)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Challenges in RAG (Generation Bottleneck)","Context Utilization (Lost-in-the-middle phenomenon)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Retrieval-Augmented Generation (RAG)","Challenges in RAG (Generation Bottleneck)","Source Attribution/Faithful Decoding (Ensuring output aligns with retrieved source)",""
"Mitigating Hallucination in Large Language Models (LLMs)","General Hallucination Mitigation Strategies","Data-Related Mitigation","Data Filtering (Select high-quality data, deduplication)",""
"Mitigating Hallucination in Large Language Models (LLMs)","General Hallucination Mitigation Strategies","Data-Related Mitigation","Model Editing (Rectify specific misinformation by updating parameters)",""
"Mitigating Hallucination in Large Language Models (LLMs)","General Hallucination Mitigation Strategies","Data-Related Mitigation","Retrieval-Augmented Generation (Use external knowledge bases)",""
"Mitigating Hallucination in Large Language Models (LLMs)","General Hallucination Mitigation Strategies","Training-Related Mitigation","Pre-training (Address unidirectional limits, attention glitches)",""
"Mitigating Hallucination in Large Language Models (LLMs)","General Hallucination Mitigation Strategies","Training-Related Mitigation","Misalignment Hallucination (Address sycophancy via improved RLHF/Activation Steering)",""
"Mitigating Hallucination in Large Language Models (LLMs)","General Hallucination Mitigation Strategies","Inference-Related Mitigation","Factuality Enhanced Decoding (Dynamically adjust sampling to prioritize facts, ITI, DoLa)",""
"Mitigating Hallucination in Large Language Models (LLMs)","General Hallucination Mitigation Strategies","Inference-Related Mitigation","Faithfulness Enhanced Decoding (Context-aware decoding, self-consistency in CoT)",""
"Mitigating Hallucination in Large Language Models (LLMs)","General Hallucination Mitigation Strategies","Inference-Related Mitigation","Post-editing Decoding (Self-correction/self-reflection, e.g., CoVe)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Detection Categories","Factuality Hallucination Detection","Fact-checking (External Retrieval, Internal Checking/CoVe)"
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Detection Categories","Factuality Hallucination Detection","Uncertainty Estimation (LLM internal states, LLM behavior/SelfCheckGPT)"
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Detection Categories","Faithfulness Hallucination Detection","Fact-based Metrics (N-gram, Entity, Relation overlap)"
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Detection Categories","Faithfulness Hallucination Detection","QA-based Metrics (Question generation/answering to validate consistency)"
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Detection Categories","Faithfulness Hallucination Detection","LLM-based Judgement (LLMs as evaluators via prompting)"
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Commercial Detection Tools (2025)","Future AGI (Integrated monitoring, context adherence checks)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Commercial Detection Tools (2025)","Pythia (Knowledge graph-based fact-checking)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Commercial Detection Tools (2025)","Galileo (Hallucination Index, real-time blocking)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Commercial Detection Tools (2025)","Cleanlab (Uncertainty metrics, trust scoring)",""
"Mitigating Hallucination in Large Language Models (LLMs)","Hallucination Detection & Monitoring","Commercial Detection Tools (2025)","Patronus AI (Open-source, explainable feedback)",""
